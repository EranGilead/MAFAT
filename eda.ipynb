{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MAFAT 2025 Semantic Retrieval Competition - Exploratory Data Analysis\n",
        "\n",
        "This notebook performs exploratory data analysis on the Israeli semantic retrieval competition dataset.\n",
        "\n",
        "## Import Libraries and Setup\n",
        "\n",
        "First, we'll import all necessary libraries for data analysis and visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w7SjlxbhAjnq"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSdcY2vwxEgt"
      },
      "source": [
        "## Download training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Loading\n",
        "\n",
        "Load the JSONL dataset containing query-paragraph pairs with relevance scores for the semantic retrieval competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_path = './hsrc/hsrc_train.jsonl'\n",
        "rows = []\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        rows.append(json.loads(line))\n",
        "\n",
        "df = pd.DataFrame(rows)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O28Map8FwcUF"
      },
      "source": [
        "Select the data for analysis and display.\n",
        "(note that running the notebook on the full dataset will not be possible in a standard collab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bed4tGX4-hOp"
      },
      "source": [
        "## EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ob-aW8SjbxL"
      },
      "source": [
        "### Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61ib0pKS-mEz"
      },
      "source": [
        "The competition's data consists of the following columns:\n",
        "1. query_uuid - unique identifier for query\n",
        "2. query - query text\n",
        "3. paragraphs - list of paragraphs with uuid and passage for each one\n",
        "4. target_actions - list of target actions for each paragraph and query\n",
        "5. case_name - data corpuses - source\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initial Data Exploration\n",
        "\n",
        "Let's first examine the structure of our dataset by looking at the column names and basic information.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Preview\n",
        "\n",
        "Display the first few rows to understand the data structure and content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "id": "jkAV_A8iSHWo",
        "outputId": "d954eec8-afaa-4fbb-ffeb-dddf03502f99"
      },
      "outputs": [],
      "source": [
        "data_head = df.head()\n",
        "display(data_head)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Composition\n",
        "\n",
        "Examine the different case names (corpuses) present in the dataset to understand the variety of content.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the unique corpuses in the dataset\n",
        "unique_corpuses = df['case_name'].unique()\n",
        "print(\"Unique corpuses:\", unique_corpuses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Preprocessing\n",
        "\n",
        "Create a working copy of the dataset for transformation and analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Structure Transformation\n",
        "\n",
        "The dataset contains nested dictionaries for paragraphs and target actions. We need to unwrap these structures to make the data more accessible for analysis.\n",
        "\n",
        "These functions will:\n",
        "- Extract paragraph UUIDs and text content from the nested paragraph structure\n",
        "- Extract relevance scores from the target actions structure\n",
        "- Maintain proper ordering based on paragraph indices\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def unwrap_paragraphs(par_dict):\n",
        "    # sort items by the numeric suffix of \"paragraph_i\"\n",
        "    items = sorted(\n",
        "        par_dict.items(),\n",
        "        key=lambda kv: int(kv[0].split('_')[-1])\n",
        "    )\n",
        "    uuids   = [v['uuid']    for _, v in items]\n",
        "    texts   = [v['passage'] for _, v in items]\n",
        "    return uuids, texts\n",
        "\n",
        "def unwrap_targets(tgt_dict):\n",
        "    # sort items by the numeric suffix of \"target_action_i\"\n",
        "    items = sorted(\n",
        "        tgt_dict.items(),\n",
        "        key=lambda kv: int(kv[0].split('_')[-1])\n",
        "    )\n",
        "    rels = [v for _, v in items]\n",
        "    return rels\n",
        "\n",
        "# apply to each row\n",
        "df['paragraph_uuids'], df['paragraph_texts'] = \\\n",
        "    zip(*df['paragraphs'].map(unwrap_paragraphs))\n",
        "\n",
        "df['relevances'] = df['target_actions'].map(unwrap_targets)\n",
        "\n",
        "df.rename(columns={'case_name': 'source'}, inplace=True)\n",
        "\n",
        "# sanity checks\n",
        "assert all(len(x)==20 for x in df['paragraph_uuids'])\n",
        "assert all(len(x)==20 for x in df['relevances'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Reshaping - Creating Long Format\n",
        "\n",
        "Transform the data from wide format (one row per query with 20 paragraphs) to long format (one row per query-paragraph pair). This makes it easier to analyze relationships between queries, paragraphs, and relevance scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import display, HTML\n",
        "\n",
        "# 0..19 positions\n",
        "df['position'] = df['paragraph_uuids'].map(lambda lst: list(range(len(lst))))\n",
        "\n",
        "# explode all in lock‐step\n",
        "df_long = (\n",
        "    df\n",
        "    .explode(['paragraph_uuids','paragraph_texts','relevances','position'])\n",
        "    .rename(columns={\n",
        "        'paragraph_uuids':'paragraph_uuid',\n",
        "        'paragraph_texts':'paragraph_text',\n",
        "        'relevances':'relevance'\n",
        "    })\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "# keep only the columns you need\n",
        "df_long = df_long[[\n",
        "    'query_uuid','query','source',\n",
        "    'position','paragraph_uuid','paragraph_text','relevance'\n",
        "]]\n",
        "\n",
        "print(df_long.shape)   # should be n_queries * 20 rows"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample Data by Source\n",
        "\n",
        "Display one example from each source to understand the content and structure differences across different corpuses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display 1 example per source\n",
        "examples_per_source = df_long.groupby('source').apply(lambda group: group.head(1))\n",
        "# Display the examples\n",
        "display(HTML(examples_per_source.drop(columns=['source']).style.set_table_attributes('dir=\"rtl\"').to_html(index=False, escape=False)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset Statistics\n",
        "\n",
        "Generate summary statistics to understand the scale and composition of our dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "print(\"Rows:\",      len(df_long))\n",
        "print(\"Queries:\",   df_long['query_uuid'].nunique())\n",
        "print(\"Paragraphs:\", len(df_long))\n",
        "print(\"Relevance labels:\", sorted(df_long['relevance'].unique()))\n",
        "# df_long.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Quality Check\n",
        "\n",
        "Identify any problematic relevance labels that need to be cleaned or handled specially.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wrong_relevance_labels = df_long[df_long['relevance'].isin(['', '-', '9'])]\n",
        "wrong_relevance_labels[['query_uuid','paragraph_uuid','relevance']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning\n",
        "\n",
        "Clean the relevance scores by:\n",
        "- Converting to numeric format\n",
        "- Removing invalid entries\n",
        "- Filtering to valid relevance range (0-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_long['relevance_num'] = pd.to_numeric(df_long['relevance'],\n",
        "                                         errors='coerce')\n",
        "df_long = df_long.dropna(subset=['relevance_num'])\n",
        "df_long = df_long[df_long['relevance_num'] <= 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Relevance Distribution Analysis\n",
        "\n",
        "### Overall Relevance Distribution\n",
        "\n",
        "Visualize the distribution of relevance scores across all query-paragraph pairs to understand label balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "order = sorted(df_long['relevance_num'].unique(), key=int)\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.countplot(x='relevance_num',\n",
        "              data=df_long,\n",
        "              order=order,\n",
        "              palette=\"Blues_d\")\n",
        "plt.title(\"Overall Relevance Label Counts\")\n",
        "plt.xlabel(\"Relevance (0–4)\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Relevant Paragraphs per Query by Case\n",
        "\n",
        "Analyze how many relevant paragraphs (relevance > 0) each query has, broken down by case name. This helps understand the difficulty and characteristics of different corpuses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how many relevant (relevance>0) paras per query?\n",
        "nonzero_per_q = df_long[df_long.relevance_num>0].groupby('query_uuid').size()\n",
        "# Split the queries by source\n",
        "sources = df_long['source'].unique()\n",
        "# turn the Series into a DataFrame column\n",
        "tmp = nonzero_per_q.reset_index()\n",
        "tmp.columns = ['query_uuid','n_relevant']\n",
        "# Create a plot for each source\n",
        "for case in sources:\n",
        "    color = sns.color_palette(\"husl\", len(sources))[list(sources).index(case)]\n",
        "    case_data = tmp[tmp['query_uuid'].isin(df_long[df_long['source'] == case]['query_uuid'])]\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.countplot(x='n_relevant', data=case_data, color=color)\n",
        "    plt.title(f\"Number of Paragraphs with Relevance>0 per Query ({case})\")\n",
        "    plt.xlabel(\"Count of paras with relevance>0\")\n",
        "    plt.ylabel(\"Number of Queries\")\n",
        "    plt.xticks(range(case_data['n_relevant'].min(), case_data['n_relevant'].max() + 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Distribution of paragraph length"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Choose: \"chars\" for characters, \"words\" for words\n",
        "length_type = \"words\"\n",
        "\n",
        "lengths = []\n",
        "\n",
        "# Read the JSONL file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip():\n",
        "            data = json.loads(line)\n",
        "            paragraphs = data.get(\"paragraphs\", {})\n",
        "            for para in paragraphs.values():\n",
        "                passage = para.get(\"passage\", \"\")\n",
        "                if length_type == \"chars\":\n",
        "                    lengths.append(len(passage))\n",
        "                elif length_type == \"words\":\n",
        "                    lengths.append(len(passage.split()))\n",
        "\n",
        "# Stats\n",
        "print(f\"Total passages: {len(lengths)}\")\n",
        "print(f\"Min length: {min(lengths)}\")\n",
        "print(f\"Max length: {max(lengths)}\")\n",
        "print(f\"Average length: {sum(lengths)/len(lengths):.2f}\")\n",
        "\n",
        "# Distribution table\n",
        "distribution = Counter(lengths)\n",
        "print(\"\\nSample distribution (length: count):\")\n",
        "for length, count in sorted(distribution.items())[:20]:\n",
        "    print(f\"{length}: {count}\")\n",
        "\n",
        "# Plot histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(lengths, bins=30, color='skyblue', edgecolor='black')\n",
        "plt.title(f\"Distribution of Passage Lengths ({length_type})\")\n",
        "plt.xlabel(f\"Length in {length_type}\")\n",
        "plt.ylabel(\"Number of passages\")\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Advanced Feature Analysis\n",
        "\n",
        "### Cosine Similarity Analysis\n",
        "\n",
        "Calculate cosine similarity between queries and paragraphs using TF-IDF vectorization to understand the relationship between semantic similarity and relevance scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "def calculate_cosine_similarity_me5_optimized(df, sample_size=500):\n",
        "    \"\"\"Calculate cosine similarity using ME5 with optimizations\"\"\"\n",
        "    similarities = []\n",
        "    \n",
        "    # Sample the data for faster processing\n",
        "    print(f\"Sampling {sample_size} queries from {df['query_uuid'].nunique()} total queries\")\n",
        "    sampled_queries = df['query_uuid'].unique()[:sample_size]\n",
        "    df_sample = df[df['query_uuid'].isin(sampled_queries)]\n",
        "    \n",
        "    # Use smaller, faster model\n",
        "    print(\"Loading multilingual-e5-base (faster than large)...\")\n",
        "    model = SentenceTransformer('intfloat/multilingual-e5-base')\n",
        "    print(\"Model loaded successfully!\")\n",
        "    \n",
        "    # Process in batches for efficiency\n",
        "    batch_size = 32\n",
        "    \n",
        "    for i, (query_uuid, group) in enumerate(df_sample.groupby('query_uuid')):\n",
        "        if i % 50 == 0:  # More frequent progress updates\n",
        "            print(f\"Processing query {i+1}/{len(sampled_queries)}\")\n",
        "        \n",
        "        query_text = group['query'].iloc[0]\n",
        "        paragraph_texts = group['paragraph_text'].tolist()\n",
        "        \n",
        "        # Truncate very long texts to speed up processing\n",
        "        query_text = query_text[:512] if len(query_text) > 512 else query_text\n",
        "        paragraph_texts = [text[:512] if len(text) > 512 else text for text in paragraph_texts]\n",
        "        \n",
        "        try:\n",
        "            # Add prefixes for better performance\n",
        "            query_with_prefix = f\"query: {query_text}\"\n",
        "            paragraphs_with_prefix = [f\"passage: {text}\" for text in paragraph_texts]\n",
        "            \n",
        "            # Encode query\n",
        "            query_embedding = model.encode([query_with_prefix], batch_size=1)\n",
        "            \n",
        "            # Encode paragraphs in batch\n",
        "            paragraph_embeddings = model.encode(paragraphs_with_prefix, batch_size=batch_size)\n",
        "            \n",
        "            # Calculate cosine similarity\n",
        "            cosine_sims = cosine_similarity(query_embedding, paragraph_embeddings).flatten()\n",
        "            similarities.extend(cosine_sims)\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing query {query_uuid}: {e}\")\n",
        "            similarities.extend([0.0] * len(paragraph_texts))\n",
        "    \n",
        "    return similarities, df_sample\n",
        "\n",
        "print(\"Calculating cosine similarities using optimized ME5...\")\n",
        "similarities, df_sample = calculate_cosine_similarity_me5_optimized(df_long, sample_size=500)\n",
        "df_sample['cosine_similarity_me5'] = similarities\n",
        "print(\"ME5 cosine similarity calculation completed!\")\n",
        "\n",
        "# Use the sampled data for analysis\n",
        "df_long = df_sample.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Correlation Analysis with Cosine Similarity\n",
        "\n",
        "Examine the relationship between cosine similarity and relevance scores, along with other text-based features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Update the correlation analysis to include ME5 cosine similarity\n",
        "num_cols = ['relevance_num', 'cosine_similarity_me5']\n",
        "\n",
        "# Compute the correlation matrix\n",
        "corr = df_long[num_cols].corr()\n",
        "\n",
        "# Plot a larger heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(corr, annot=True, fmt=\".3f\", cmap=\"vlag\", center=0, linewidths=.5,\n",
        "            square=True, cbar_kws={\"shrink\": .8})\n",
        "plt.title(\"Correlation Matrix with ME5 Cosine Similarity\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print features sorted by absolute correlation with relevance\n",
        "print(\"Features sorted by |correlation| with relevance:\")\n",
        "relevance_corr = corr['relevance_num'].drop('relevance_num').abs().sort_values(ascending=False)\n",
        "for feature, correlation in relevance_corr.items():\n",
        "    print(f\"{feature}: {correlation:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Cosine Similarity Distribution by Relevance\n",
        "\n",
        "Visualize how cosine similarity values are distributed across different relevance scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create box plot showing cosine similarity distribution by relevance score\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.boxplot(x='relevance_num', y='cosine_similarity_me5', data=df_long, palette=\"viridis\")\n",
        "plt.title(\"ME5 Cosine Similarity Distribution by Relevance Score\")\n",
        "plt.xlabel(\"Relevance Score\")\n",
        "plt.ylabel(\"ME5 Cosine Similarity\")\n",
        "plt.show()\n",
        "\n",
        "# Create violin plot for more detailed distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.violinplot(x='relevance_num', y='cosine_similarity_me5', data=df_long, palette=\"viridis\")\n",
        "plt.title(\"ME5 Cosine Similarity Distribution by Relevance Score (Violin Plot)\")\n",
        "plt.xlabel(\"Relevance Score\")\n",
        "plt.ylabel(\"ME5 Cosine Similarity\")\n",
        "plt.show()\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"ME5 Cosine Similarity Statistics by Relevance Score:\")\n",
        "print(df_long.groupby('relevance_num')['cosine_similarity_me5'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scatter Plot Analysis\n",
        "\n",
        "Examine the direct relationship between cosine similarity and relevance scores.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scatter plot with trend line\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x='cosine_similarity_me5', y='relevance_num', data=df_long, alpha=0.6)\n",
        "sns.regplot(x='cosine_similarity_me5', y='relevance_num', data=df_long, scatter=False, color='red')\n",
        "plt.title(\"Relationship between ME5 Cosine Similarity and Relevance Score\")\n",
        "plt.xlabel(\"ME5 Cosine Similarity\")\n",
        "plt.ylabel(\"Relevance Score\")\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display Pearson correlation coefficient\n",
        "from scipy.stats import pearsonr\n",
        "correlation, p_value = pearsonr(df_long['cosine_similarity_me5'], df_long['relevance_num'])\n",
        "print(f\"Pearson correlation between ME5 cosine similarity and relevance: {correlation:.4f}\")\n",
        "print(f\"P-value: {p_value:.4e}\")\n",
        "\n",
        "# Calculate Spearman correlation (rank-based)\n",
        "from scipy.stats import spearmanr\n",
        "spearman_corr, spearman_p = spearmanr(df_long['cosine_similarity_me5'], df_long['relevance_num'])\n",
        "print(f\"Spearman correlation between ME5 cosine similarity and relevance: {spearman_corr:.4f}\")\n",
        "print(f\"P-value: {spearman_p:.4e}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
